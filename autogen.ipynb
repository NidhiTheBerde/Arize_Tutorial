{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial : Building a Multi-Agent Chat System with Azure OpenAI, Autogen and Tracing using Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction : In this tutorial, we'll guide you through building a multi-agent chat system using Azure OpenAI deployments. The example in this tutorial is a customer query resolver agent. The system will utilize multiple AI Agents, each with a specific role, to colaboratively come up with a solution to user query. You'll see how to co-ordinate these agents in a RoundRobin Fashion, trace their interactions with Phoenic and add human annotations for debugging and optimization. The applications of multi-agent systems are very diverse. The Agents are capable of handing both TextMessage or MultiModalMessage. Autogen AgentChat provides a set of preset Agents, each with variations on how an agent might respond to messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer <a href = https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html>Autogen</a> Documentation for more details about AgentChat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prequisites : Python, Access to OpenAI models. Other dependencies are listed in requirements.txt. \n",
    "Run pip install -r /path/to/requirements.txt to have all packages installed at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is a good practice to save all api keys, endpoints and other important details in an env file. \n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(\"env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the Phoenix App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n",
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch Phoenix\n",
    "import os\n",
    "if \"PHOENIX_API_KEY\" in os.environ:\n",
    "    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "else:\n",
    "    import phoenix as px\n",
    "\n",
    "    px.launch_app().view()\n",
    "\n",
    "# Connect to Phoenix\n",
    "from phoenix.otel import register\n",
    "tracer_provider = register()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument OpenAI for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, agents need access to LLM model services such as OpenAI, Azure OpenAI, or local models. Since there are many different providers with different APIs, autogen-core implements a protocol for model clients and autogen-ext implements a set of model clients for popular model services. AgentChat can use these model clients to interact with model services.\n",
    "\n",
    "To use the Azure OpenAI, you need to provide your deployment id, Azure Cognitive Services endpoint, api version, and model capabilities. For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Set up your Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    " \n",
    "\n",
    "#Set up Azure OpenAI Configuration using environment variables\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"API_VERSION\")\n",
    "\n",
    " \n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"model_name\",\n",
    "    model=\"model_name\",\n",
    "    api_version=\"your_model_version\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Define Agent Functions\n",
    "Each Agent is initialized using the AssistantAgent class from Autogen. It's up to the developer to decide the system message according to their needs\n",
    "Here I have used three agents - Classifier, Resolver and Feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination # type: ignore\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat \n",
    "\n",
    "classifier_agent = AssistantAgent(\n",
    "\n",
    "    name=\"classifier\",\n",
    "    model_client =az_model_client,\n",
    "\n",
    "    system_message=(\"You are a classifier that determines the category og a customer query\"\n",
    "                    \"The categories are Billing, Technical Support, Shipping or General Inquiry\"\n",
    "                    \"Respond only with categoy name\")\n",
    ")\n",
    "\n",
    "\n",
    "problem_solver_agent = AssistantAgent(\n",
    "\n",
    "    name=\"problem_solver\",\n",
    "\n",
    "    model_client = az_model_client,\n",
    "\n",
    "    system_message=(\"You are a resolver that answers customer queries based on their category.\"\n",
    "                    \"You will receive the category and query, and you must provide a resolution\"\n",
    "                    \"Be concise, clear, and empatheic in your response\"\n",
    "                    \"Try to limit the answers in 5 lines\")\n",
    "\n",
    ")\n",
    "\n",
    "feedbackagent = AssistantAgent(\n",
    "    name = \"feedback_agent\",\n",
    "     model_client=az_model_client,\n",
    "    system_message=(\n",
    "        \"You analyse customer interactions for sentiments and suggest improvements\"\n",
    "        \"If the sentiment is negative, identify and propose better response\"\n",
    "        \"If all looks good respond with TERMINATE\"\n",
    "    )\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Now we set up a team using the RoundRobinGroupChat configuration. This configuration allows multiple agents to work together in a round-robin fashion, where each agent takes turns responding while maintaining a shared context.\n",
    "\n",
    "The AssistantAgent is responsible for generating responses based on the given input. Additionally, we will set a TextMentionTermination condition that stops the team when a specific word is detected in any of the agent's responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination =TextMentionTermination(\"TERMINATE\")\n",
    "group_chat1 = RoundRobinGroupChat(\n",
    "    [classifier_agent, problem_solver_agent, feedbackagent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import TraceDataset\n",
    "from autogen_agentchat.ui import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='who do i contact for broken screen?', type='TextMessage'), TextMessage(source='classifier', models_usage=RequestUsage(prompt_tokens=51, completion_tokens=2), content='Technical Support', type='TextMessage'), TextMessage(source='problem_solver', models_usage=RequestUsage(prompt_tokens=76, completion_tokens=55), content=\"I'm sorry to hear about your broken screen. To get it fixed, please contact the technical support team of the device's manufacturer. If you have warranty or insurance, make sure to mention that when you reach out. They will guide you through the repair or replacement process.\", type='TextMessage'), TextMessage(source='feedback_agent', models_usage=RequestUsage(prompt_tokens=118, completion_tokens=3), content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who do i contact for broken screen?\"\n",
    "await Console(group_chat1.run_stream(task=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_chat2 = RoundRobinGroupChat(\n",
    "    [classifier_agent, problem_solver_agent, feedbackagent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Query: My payment isnt going through on your app', type='TextMessage'), TextMessage(source='classifier', models_usage=RequestUsage(prompt_tokens=74, completion_tokens=1), content='Billing', type='TextMessage'), TextMessage(source='problem_solver', models_usage=RequestUsage(prompt_tokens=160, completion_tokens=62), content=\"I apologize for the inconvenience you're experiencing with the payment. Please ensure your card details are entered correctly and that your card is not expired. If the issue persists, try using a different payment method or contact our billing support for further assistance. They will be able to help resolve any payment issues you're facing.\", type='TextMessage'), TextMessage(source='feedback_agent', models_usage=RequestUsage(prompt_tokens=219, completion_tokens=3), content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"My payment isnt going through on your app\"\n",
    "await Console(group_chat2.run_stream(task=f\"Query: {query1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n",
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:330: UserWarning: Resolved model mismatch: gpt-4-0613 != gpt-4. Model mapping may be incorrect.\n",
      "  result = await self._model_client.create(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Query: I cant log into my account', type='TextMessage'), TextMessage(source='classifier', models_usage=RequestUsage(prompt_tokens=173, completion_tokens=2), content='Technical Support', type='TextMessage'), TextMessage(source='problem_solver', models_usage=RequestUsage(prompt_tokens=260, completion_tokens=71), content=\"I'm sorry to hear you're having trouble logging in. Please check if you're using the correct username and password. If you've forgotten your password, use the 'Forgot Password' feature to reset it. If the problem persists, please reach out to our technical support team for assistance. They'll be happy to help you regain access to your account.\", type='TextMessage'), TextMessage(source='feedback_agent', models_usage=RequestUsage(prompt_tokens=328, completion_tokens=3), content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = \"I cant log into my account\"\n",
    "await Console(group_chat2.run_stream(task=f\"Query: {query2}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 : Open the phoenix url that opened in your localhost, you will now see the traces for all the queries that we ran.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Trace dataset saved to under ID: 8aa487d2-7cfb-4dc0-a762-9d134f6a5dd8\n",
      "üìÇ Trace dataset path: /home/codespace/.phoenix/trace_datasets/trace_dataset-8aa487d2-7cfb-4dc0-a762-9d134f6a5dd8.parquet\n"
     ]
    }
   ],
   "source": [
    "my_traces = px.Client().get_trace_dataset().save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('a4290b62-2880-45b8-af9b-d5db4fde4323')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x721cdaffb950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.launch_app(trace=px.TraceDataset.load(my_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736784018.404160    1550 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
