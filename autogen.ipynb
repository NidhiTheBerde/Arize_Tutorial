{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial : Building a Multi-Agent Chat System with Azure OpenAI, Autogen and Tracing using Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through building a multi-agent chat system using Azure OpenAI deployments, Autogen, and Phoenix for tracing and debugging. The example scenario is a customer query resolver system, where multiple AI agents, each with a specialized role, collaboratively address user queries. You will learn how to coordinate these agents in a Round-Robin fashion, trace their interactions using Phoenix, and incorporate human annotations for debugging, optimization, and improvement.\n",
    "\n",
    "The applications of multi-agent systems are broad, ranging from customer support and recommendation engines to dynamic problem-solving workflows. This tutorial highlights how to leverage Autogen's AgentChat framework, which provides a set of preconfigured agents with variations in behavior and response strategies. The agents in this example are capable of handling both text messages and multi-modal messages, making the system versatile and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Autogen‚Äôs AgentChat capabilities and configurations, refer to the <a href = https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html>Autogen Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites\n",
    "\n",
    "Programming Language: Python (version 3.8 or later recommended).\n",
    "Azure OpenAI Access: Ensure you have access to OpenAI models via Azure OpenAI Service, including necessary credentials and deployment configurations.\n",
    "Required Dependencies:\n",
    "Install all necessary Python packages by running the following command:\n",
    "pip install -r /path/to/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is a good practice to save all api keys, endpoints and other important details in an env file. \n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(\"env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the Phoenix App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Arize_Tutorial/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n",
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch Phoenix\n",
    "import os\n",
    "if \"PHOENIX_API_KEY\" in os.environ:\n",
    "    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "else:\n",
    "    import phoenix as px\n",
    "\n",
    "    px.launch_app().view()\n",
    "\n",
    "# Connect to Phoenix\n",
    "from phoenix.otel import register\n",
    "tracer_provider = register()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument OpenAI for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Model Clients in Autogen\n",
    "In multi-agent systems, agents often rely on external LLMs to process and generate responses. Since each LLM provider (e.g., OpenAI, Azure OpenAI, local models) has distinct APIs, managing these differences can be challenging. To address this, autogen-core defines a protocol for model clients, providing a consistent interface for interacting with LLM services.\n",
    "\n",
    "To complement this, autogen-ext includes prebuilt model clients for popular LLM providers, such as:\n",
    "\n",
    "OpenAI GPT models: Standard GPT models hosted on OpenAI's API.\n",
    "Azure OpenAI: GPT models deployed on Microsoft's Azure platform.\n",
    "Local models: Custom LLMs deployed locally, allowing offline access.\n",
    "These abstractions allow AgentChat to work seamlessly with different model backends, ensuring flexibility and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Azure OpenAI with AgentChat\n",
    "When using Azure OpenAI with AgentChat, you need to configure the model client to interact with your Azure Cognitive Services deployment. Here‚Äôs a breakdown of the required settings:\n",
    "\n",
    "1. Deployment ID\n",
    "The unique identifier for your deployed Azure OpenAI model.\n",
    "Example: gpt-4-deployment.\n",
    "2. Azure Cognitive Services Endpoint\n",
    "The base URL for your Azure Cognitive Services resource.\n",
    "Example: https://<your-resource-name>.openai.azure.com/.\n",
    "3. API Version\n",
    "Specifies the API version to use. Ensure compatibility with your Azure deployment.\n",
    "Example: 2023-05-15.\n",
    "4. Model Capabilities\n",
    "Describes the specific functions of the model, such as text generation, summarization, or classification.\n",
    "5. Authentication\n",
    "Two methods are supported for authentication:\n",
    "API Key: Use the AZURE_OPENAI_API_KEY environment variable to provide the key.\n",
    "Azure Active Directory (AAD): Use an AAD token credential for enhanced security and role-based access control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Set up your Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    " \n",
    "\n",
    "#Set up Azure OpenAI Configuration using environment variables\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"API_VERSION\")\n",
    "\n",
    " \n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"model_name\",\n",
    "    model=\"model_name\",\n",
    "    api_version=\"your_model_version\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Define Agent Functions\n",
    "\n",
    "In this step, we define the functions for each agent in our multi-agent system. Each agent is initialized using the AssistantAgent class from Autogen. The agents' behavior is primarily determined by their system messages, which define the roles, responsibilities, and tone of each agent.\n",
    "\n",
    "For this tutorial, we'll focus on three distinct agents:\n",
    "\n",
    "Classifier Agent: Identifies the type of user query.\n",
    "Resolver Agent: Provides a solution to the query.\n",
    "Feedback Agent: Evaluates the resolution and suggests improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination # type: ignore\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat \n",
    "\n",
    "classifier_agent = AssistantAgent(\n",
    "\n",
    "    name=\"classifier\",\n",
    "    model_client =az_model_client,\n",
    "\n",
    "    system_message=(\"You are a classifier that determines the category og a customer query\"\n",
    "                    \"The categories are Billing, Technical Support, Shipping or General Inquiry\"\n",
    "                    \"Respond only with categoy name\")\n",
    ")\n",
    "\n",
    "\n",
    "problem_solver_agent = AssistantAgent(\n",
    "\n",
    "    name=\"problem_solver\",\n",
    "\n",
    "    model_client = az_model_client,\n",
    "\n",
    "    system_message=(\"You are a resolver that answers customer queries based on their category.\"\n",
    "                    \"You will receive the category and query, and you must provide a resolution\"\n",
    "                    \"Be concise, clear, and empatheic in your response\"\n",
    "                    \"Try to limit the answers in 5 lines\")\n",
    "\n",
    ")\n",
    "\n",
    "feedbackagent = AssistantAgent(\n",
    "    name = \"feedback_agent\",\n",
    "     model_client=az_model_client,\n",
    "    system_message=(\n",
    "        \"You analyse customer interactions for sentiments and suggest improvements\"\n",
    "        \"If the sentiment is negative, identify and propose better response\"\n",
    "        \"If all looks good respond with TERMINATE\"\n",
    "    )\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Setting Up a Team with RoundRobinGroupChat\n",
    "\n",
    "In this step, we configure a RoundRobinGroupChat to enable multiple agents to work collaboratively. This configuration ensures that each agent takes turns responding to user queries while maintaining a shared conversation context. To manage the flow of the conversation effectively, we'll also implement a TextMentionTermination condition, which stops the interaction when a specific keyword or phrase is detected in any agent's response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination =TextMentionTermination(\"TERMINATE\")\n",
    "group_chat1 = RoundRobinGroupChat(\n",
    "    [classifier_agent, problem_solver_agent, feedbackagent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import TraceDataset\n",
    "from autogen_agentchat.ui import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who do i contact for broken screen?\"\n",
    "await Console(group_chat1.run_stream(task=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"My payment isnt going through on your app\"\n",
    "await Console(group_chat2.run_stream(task=f\"Query: {query1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"I cant log into my account\"\n",
    "await Console(group_chat2.run_stream(task=f\"Query: {query2}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Open the phoenix url that opened in your localhost, you will now see the traces for all the queries that we ran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Adding Human Annotations\n",
    "\n",
    "In this step, we'll explore how to add human annotations to traces in Phoenix. Human annotations serve as an essential tool for improving LLM-driven applications by capturing qualitative insights, curating datasets, and ensuring the application‚Äôs continuous improvement.\n",
    "\n",
    "Annotations in Phoenix are simple yet effective. They allow you to assign labels or scores to specific spans (segments of a trace). These annotations can be used for debugging, sharing feedback within a team, and creating datasets for fine-tuning or training LLMs.\n",
    "\n",
    "Purpose of Human Annotations\n",
    "Quality Assurance: Provide a secondary layer of validation by tagging responses that meet or fail to meet quality expectations.\n",
    "Dataset Curation: Identify good or bad examples to build curated datasets for further training.\n",
    "Collaborative Debugging: Share annotations across teams to foster collaborative debugging and optimization.\n",
    "LLM Judge Training: Use annotated spans to train LLMs to evaluate future interactions more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply click on the Trace and then click on 'Annotate'. Next give a suitable name to your annotation, provide label and a score for your reference. Refer the images below or visit this <a href=https://docs.arize.com/phoenix/tracing/llm-traces/how-to-annotate-traces>link</a> for a quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done they look like this\n",
    "\n",
    "<img src = \"img3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more than one annotation for each trace\n",
    "\n",
    "<img src=\"img6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also Filter your traces based on the annotations, or any other filter conditions to easliy view your traces. Click on the '+' in the search bar and add the filter\n",
    "\n",
    "<img src = \"img5.png\">\n",
    "<img src = \"img4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Save and Load Traces for Reuse\n",
    "\n",
    "Saving and loading traces is a critical aspect of maintaining a robust and iterative development process when working with multi-agent systems. Phoenix provides functionality to persist traces, allowing you to analyze them later, share them across teams, or use them as benchmarks in future evaluations.\n",
    "\n",
    "Why Save and Load Traces?\n",
    "Reusability: Analyze historical interactions to identify trends or recurring issues.\n",
    "Collaboration: Share traces with team members for debugging or quality assurance.\n",
    "Benchmarking: Use traces to compare the performance of newer iterations or models.\n",
    "Scalability: Persist and manage traces efficiently for large-scale systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_traces = px.Client().get_trace_dataset().save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x721cdaffb950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.launch_app(trace=px.TraceDataset.load(my_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
